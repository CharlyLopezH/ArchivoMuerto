{"ast":null,"code":"/**\n * @typedef {import('micromark-util-types').Construct} Construct\n * @typedef {import('micromark-util-types').Resolver} Resolver\n * @typedef {import('micromark-util-types').Tokenizer} Tokenizer\n * @typedef {import('micromark-util-types').Token} Token\n * @typedef {import('micromark-util-types').State} State\n */\nimport { ok as assert } from 'uvu/assert';\nimport { factorySpace } from 'micromark-factory-space';\nimport { markdownLineEnding } from 'micromark-util-character';\nimport { subtokenize } from 'micromark-util-subtokenize';\nimport { codes } from 'micromark-util-symbol/codes.js';\nimport { constants } from 'micromark-util-symbol/constants.js';\nimport { types } from 'micromark-util-symbol/types.js';\n/**\n * No name because it must not be turned off.\n * @type {Construct}\n */\n\nexport const content = {\n  tokenize: tokenizeContent,\n  resolve: resolveContent\n};\n/** @type {Construct} */\n\nconst continuationConstruct = {\n  tokenize: tokenizeContinuation,\n  partial: true\n};\n/**\n * Content is transparent: it’s parsed right now. That way, definitions are also\n * parsed right now: before text in paragraphs (specifically, media) are parsed.\n *\n * @type {Resolver}\n */\n\nfunction resolveContent(events) {\n  subtokenize(events);\n  return events;\n}\n/** @type {Tokenizer} */\n\n\nfunction tokenizeContent(effects, ok) {\n  /** @type {Token} */\n  let previous;\n  return start;\n  /** @type {State} */\n\n  function start(code) {\n    assert(code !== codes.eof && !markdownLineEnding(code), 'expected no eof or eol');\n    effects.enter(types.content);\n    previous = effects.enter(types.chunkContent, {\n      contentType: constants.contentTypeContent\n    });\n    return data(code);\n  }\n  /** @type {State} */\n\n\n  function data(code) {\n    if (code === codes.eof) {\n      return contentEnd(code);\n    }\n\n    if (markdownLineEnding(code)) {\n      return effects.check(continuationConstruct, contentContinue, contentEnd)(code);\n    } // Data.\n\n\n    effects.consume(code);\n    return data;\n  }\n  /** @type {State} */\n\n\n  function contentEnd(code) {\n    effects.exit(types.chunkContent);\n    effects.exit(types.content);\n    return ok(code);\n  }\n  /** @type {State} */\n\n\n  function contentContinue(code) {\n    assert(markdownLineEnding(code), 'expected eol');\n    effects.consume(code);\n    effects.exit(types.chunkContent);\n    previous.next = effects.enter(types.chunkContent, {\n      contentType: constants.contentTypeContent,\n      previous\n    });\n    previous = previous.next;\n    return data;\n  }\n}\n/** @type {Tokenizer} */\n\n\nfunction tokenizeContinuation(effects, ok, nok) {\n  const self = this;\n  return startLookahead;\n  /** @type {State} */\n\n  function startLookahead(code) {\n    assert(markdownLineEnding(code), 'expected a line ending');\n    effects.exit(types.chunkContent);\n    effects.enter(types.lineEnding);\n    effects.consume(code);\n    effects.exit(types.lineEnding);\n    return factorySpace(effects, prefixed, types.linePrefix);\n  }\n  /** @type {State} */\n\n\n  function prefixed(code) {\n    if (code === codes.eof || markdownLineEnding(code)) {\n      return nok(code);\n    }\n\n    const tail = self.events[self.events.length - 1];\n\n    if (!self.parser.constructs.disable.null.includes('codeIndented') && tail && tail[1].type === types.linePrefix && tail[2].sliceSerialize(tail[1], true).length >= constants.tabSize) {\n      return ok(code);\n    }\n\n    return effects.interrupt(self.parser.constructs.flow, nok, ok)(code);\n  }\n}","map":{"version":3,"sources":["C:/reactjs/projects/contra/archivo/react-archivo/node_modules/micromark-core-commonmark/dev/lib/content.js"],"names":["ok","assert","factorySpace","markdownLineEnding","subtokenize","codes","constants","types","content","tokenize","tokenizeContent","resolve","resolveContent","continuationConstruct","tokenizeContinuation","partial","events","effects","previous","start","code","eof","enter","chunkContent","contentType","contentTypeContent","data","contentEnd","check","contentContinue","consume","exit","next","nok","self","startLookahead","lineEnding","prefixed","linePrefix","tail","length","parser","constructs","disable","null","includes","type","sliceSerialize","tabSize","interrupt","flow"],"mappings":"AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA,SAAQA,EAAE,IAAIC,MAAd,QAA2B,YAA3B;AACA,SAAQC,YAAR,QAA2B,yBAA3B;AACA,SAAQC,kBAAR,QAAiC,0BAAjC;AACA,SAAQC,WAAR,QAA0B,4BAA1B;AACA,SAAQC,KAAR,QAAoB,gCAApB;AACA,SAAQC,SAAR,QAAwB,oCAAxB;AACA,SAAQC,KAAR,QAAoB,gCAApB;AAEA;AACA;AACA;AACA;;AACA,OAAO,MAAMC,OAAO,GAAG;AAACC,EAAAA,QAAQ,EAAEC,eAAX;AAA4BC,EAAAA,OAAO,EAAEC;AAArC,CAAhB;AAEP;;AACA,MAAMC,qBAAqB,GAAG;AAACJ,EAAAA,QAAQ,EAAEK,oBAAX;AAAiCC,EAAAA,OAAO,EAAE;AAA1C,CAA9B;AAEA;AACA;AACA;AACA;AACA;AACA;;AACA,SAASH,cAAT,CAAwBI,MAAxB,EAAgC;AAC9BZ,EAAAA,WAAW,CAACY,MAAD,CAAX;AACA,SAAOA,MAAP;AACD;AAED;;;AACA,SAASN,eAAT,CAAyBO,OAAzB,EAAkCjB,EAAlC,EAAsC;AACpC;AACA,MAAIkB,QAAJ;AAEA,SAAOC,KAAP;AAEA;;AACA,WAASA,KAAT,CAAeC,IAAf,EAAqB;AACnBnB,IAAAA,MAAM,CACJmB,IAAI,KAAKf,KAAK,CAACgB,GAAf,IAAsB,CAAClB,kBAAkB,CAACiB,IAAD,CADrC,EAEJ,wBAFI,CAAN;AAKAH,IAAAA,OAAO,CAACK,KAAR,CAAcf,KAAK,CAACC,OAApB;AACAU,IAAAA,QAAQ,GAAGD,OAAO,CAACK,KAAR,CAAcf,KAAK,CAACgB,YAApB,EAAkC;AAC3CC,MAAAA,WAAW,EAAElB,SAAS,CAACmB;AADoB,KAAlC,CAAX;AAGA,WAAOC,IAAI,CAACN,IAAD,CAAX;AACD;AAED;;;AACA,WAASM,IAAT,CAAcN,IAAd,EAAoB;AAClB,QAAIA,IAAI,KAAKf,KAAK,CAACgB,GAAnB,EAAwB;AACtB,aAAOM,UAAU,CAACP,IAAD,CAAjB;AACD;;AAED,QAAIjB,kBAAkB,CAACiB,IAAD,CAAtB,EAA8B;AAC5B,aAAOH,OAAO,CAACW,KAAR,CACLf,qBADK,EAELgB,eAFK,EAGLF,UAHK,EAILP,IAJK,CAAP;AAKD,KAXiB,CAalB;;;AACAH,IAAAA,OAAO,CAACa,OAAR,CAAgBV,IAAhB;AACA,WAAOM,IAAP;AACD;AAED;;;AACA,WAASC,UAAT,CAAoBP,IAApB,EAA0B;AACxBH,IAAAA,OAAO,CAACc,IAAR,CAAaxB,KAAK,CAACgB,YAAnB;AACAN,IAAAA,OAAO,CAACc,IAAR,CAAaxB,KAAK,CAACC,OAAnB;AACA,WAAOR,EAAE,CAACoB,IAAD,CAAT;AACD;AAED;;;AACA,WAASS,eAAT,CAAyBT,IAAzB,EAA+B;AAC7BnB,IAAAA,MAAM,CAACE,kBAAkB,CAACiB,IAAD,CAAnB,EAA2B,cAA3B,CAAN;AACAH,IAAAA,OAAO,CAACa,OAAR,CAAgBV,IAAhB;AACAH,IAAAA,OAAO,CAACc,IAAR,CAAaxB,KAAK,CAACgB,YAAnB;AACAL,IAAAA,QAAQ,CAACc,IAAT,GAAgBf,OAAO,CAACK,KAAR,CAAcf,KAAK,CAACgB,YAApB,EAAkC;AAChDC,MAAAA,WAAW,EAAElB,SAAS,CAACmB,kBADyB;AAEhDP,MAAAA;AAFgD,KAAlC,CAAhB;AAIAA,IAAAA,QAAQ,GAAGA,QAAQ,CAACc,IAApB;AACA,WAAON,IAAP;AACD;AACF;AAED;;;AACA,SAASZ,oBAAT,CAA8BG,OAA9B,EAAuCjB,EAAvC,EAA2CiC,GAA3C,EAAgD;AAC9C,QAAMC,IAAI,GAAG,IAAb;AAEA,SAAOC,cAAP;AAEA;;AACA,WAASA,cAAT,CAAwBf,IAAxB,EAA8B;AAC5BnB,IAAAA,MAAM,CAACE,kBAAkB,CAACiB,IAAD,CAAnB,EAA2B,wBAA3B,CAAN;AACAH,IAAAA,OAAO,CAACc,IAAR,CAAaxB,KAAK,CAACgB,YAAnB;AACAN,IAAAA,OAAO,CAACK,KAAR,CAAcf,KAAK,CAAC6B,UAApB;AACAnB,IAAAA,OAAO,CAACa,OAAR,CAAgBV,IAAhB;AACAH,IAAAA,OAAO,CAACc,IAAR,CAAaxB,KAAK,CAAC6B,UAAnB;AACA,WAAOlC,YAAY,CAACe,OAAD,EAAUoB,QAAV,EAAoB9B,KAAK,CAAC+B,UAA1B,CAAnB;AACD;AAED;;;AACA,WAASD,QAAT,CAAkBjB,IAAlB,EAAwB;AACtB,QAAIA,IAAI,KAAKf,KAAK,CAACgB,GAAf,IAAsBlB,kBAAkB,CAACiB,IAAD,CAA5C,EAAoD;AAClD,aAAOa,GAAG,CAACb,IAAD,CAAV;AACD;;AAED,UAAMmB,IAAI,GAAGL,IAAI,CAAClB,MAAL,CAAYkB,IAAI,CAAClB,MAAL,CAAYwB,MAAZ,GAAqB,CAAjC,CAAb;;AAEA,QACE,CAACN,IAAI,CAACO,MAAL,CAAYC,UAAZ,CAAuBC,OAAvB,CAA+BC,IAA/B,CAAoCC,QAApC,CAA6C,cAA7C,CAAD,IACAN,IADA,IAEAA,IAAI,CAAC,CAAD,CAAJ,CAAQO,IAAR,KAAiBvC,KAAK,CAAC+B,UAFvB,IAGAC,IAAI,CAAC,CAAD,CAAJ,CAAQQ,cAAR,CAAuBR,IAAI,CAAC,CAAD,CAA3B,EAAgC,IAAhC,EAAsCC,MAAtC,IAAgDlC,SAAS,CAAC0C,OAJ5D,EAKE;AACA,aAAOhD,EAAE,CAACoB,IAAD,CAAT;AACD;;AAED,WAAOH,OAAO,CAACgC,SAAR,CAAkBf,IAAI,CAACO,MAAL,CAAYC,UAAZ,CAAuBQ,IAAzC,EAA+CjB,GAA/C,EAAoDjC,EAApD,EAAwDoB,IAAxD,CAAP;AACD;AACF","sourcesContent":["/**\n * @typedef {import('micromark-util-types').Construct} Construct\n * @typedef {import('micromark-util-types').Resolver} Resolver\n * @typedef {import('micromark-util-types').Tokenizer} Tokenizer\n * @typedef {import('micromark-util-types').Token} Token\n * @typedef {import('micromark-util-types').State} State\n */\n\nimport {ok as assert} from 'uvu/assert'\nimport {factorySpace} from 'micromark-factory-space'\nimport {markdownLineEnding} from 'micromark-util-character'\nimport {subtokenize} from 'micromark-util-subtokenize'\nimport {codes} from 'micromark-util-symbol/codes.js'\nimport {constants} from 'micromark-util-symbol/constants.js'\nimport {types} from 'micromark-util-symbol/types.js'\n\n/**\n * No name because it must not be turned off.\n * @type {Construct}\n */\nexport const content = {tokenize: tokenizeContent, resolve: resolveContent}\n\n/** @type {Construct} */\nconst continuationConstruct = {tokenize: tokenizeContinuation, partial: true}\n\n/**\n * Content is transparent: it’s parsed right now. That way, definitions are also\n * parsed right now: before text in paragraphs (specifically, media) are parsed.\n *\n * @type {Resolver}\n */\nfunction resolveContent(events) {\n  subtokenize(events)\n  return events\n}\n\n/** @type {Tokenizer} */\nfunction tokenizeContent(effects, ok) {\n  /** @type {Token} */\n  let previous\n\n  return start\n\n  /** @type {State} */\n  function start(code) {\n    assert(\n      code !== codes.eof && !markdownLineEnding(code),\n      'expected no eof or eol'\n    )\n\n    effects.enter(types.content)\n    previous = effects.enter(types.chunkContent, {\n      contentType: constants.contentTypeContent\n    })\n    return data(code)\n  }\n\n  /** @type {State} */\n  function data(code) {\n    if (code === codes.eof) {\n      return contentEnd(code)\n    }\n\n    if (markdownLineEnding(code)) {\n      return effects.check(\n        continuationConstruct,\n        contentContinue,\n        contentEnd\n      )(code)\n    }\n\n    // Data.\n    effects.consume(code)\n    return data\n  }\n\n  /** @type {State} */\n  function contentEnd(code) {\n    effects.exit(types.chunkContent)\n    effects.exit(types.content)\n    return ok(code)\n  }\n\n  /** @type {State} */\n  function contentContinue(code) {\n    assert(markdownLineEnding(code), 'expected eol')\n    effects.consume(code)\n    effects.exit(types.chunkContent)\n    previous.next = effects.enter(types.chunkContent, {\n      contentType: constants.contentTypeContent,\n      previous\n    })\n    previous = previous.next\n    return data\n  }\n}\n\n/** @type {Tokenizer} */\nfunction tokenizeContinuation(effects, ok, nok) {\n  const self = this\n\n  return startLookahead\n\n  /** @type {State} */\n  function startLookahead(code) {\n    assert(markdownLineEnding(code), 'expected a line ending')\n    effects.exit(types.chunkContent)\n    effects.enter(types.lineEnding)\n    effects.consume(code)\n    effects.exit(types.lineEnding)\n    return factorySpace(effects, prefixed, types.linePrefix)\n  }\n\n  /** @type {State} */\n  function prefixed(code) {\n    if (code === codes.eof || markdownLineEnding(code)) {\n      return nok(code)\n    }\n\n    const tail = self.events[self.events.length - 1]\n\n    if (\n      !self.parser.constructs.disable.null.includes('codeIndented') &&\n      tail &&\n      tail[1].type === types.linePrefix &&\n      tail[2].sliceSerialize(tail[1], true).length >= constants.tabSize\n    ) {\n      return ok(code)\n    }\n\n    return effects.interrupt(self.parser.constructs.flow, nok, ok)(code)\n  }\n}\n"]},"metadata":{},"sourceType":"module"}